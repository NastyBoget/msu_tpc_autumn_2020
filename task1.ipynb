{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тип документа\n",
    "\n",
    "Тип документа указывается в шапке документа (в верхней части документа).\n",
    "\n",
    "В рамках задания документ может относиться к одному из следующих типов:\n",
    "\n",
    "* «федеральный закон»\n",
    "* «постановление»\n",
    "* «приказ»\n",
    "* «распоряжение»\n",
    "* «закон»\n",
    "* «указ»\n",
    "\n",
    "Обратите внимание, что в перечне присутствуют два похожих, но отдельных типа: «закон» и «федеральный закон».\n",
    "\n",
    "Поле type должно содержать строку, обозначающую какой-то один из указанных типов, это задача классификации.\n",
    "\n",
    "**Оценка качества:** Требуется предсказать один из шести классов. Метрика качества – макро f1-мера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки:\n",
    "\n",
    "* количество \"федеральный\", \"закон\", \"постановление\", \"приказ\", \"распоряжение\", \"указ\", \"федеральный закон\" в тексте;\n",
    "* позиция, в которой встретилось каждое из этих слов;\n",
    "* бинарный признак - если какое-то из слов встретилось и написано капсом;\n",
    "* бинарный признак - если слово единственное в данной строке (учитываем слова из букв);\n",
    "* бинарный признак - если слово первое в строке (документе);\n",
    "* учет опечаток в словах??? - м. б. находить расстояние между словами (если слово написано капсом и одно в строке, то можно его сравнить со словами из нашего множества)\n",
    "* можно добавить еще поиск слов \"постановляет\" и т. д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exprs = [\n",
    "    re.compile(r'[Ф|ф]едеральный [З|з]акон'),\n",
    "    re.compile(r'ФЕДЕРАЛЬНЫЙ ЗАКОН'),\n",
    "    re.compile(r'[П|п]остановление'),\n",
    "    re.compile(r'ПОСТАНОВЛЕНИЕ'),\n",
    "    re.compile(r'[З|з]акон'),\n",
    "    re.compile(r'ЗАКОН'),\n",
    "    re.compile(r'[П|п]риказ'),\n",
    "    re.compile(r'ПРИКАЗ'),\n",
    "    re.compile(r'[Р|р]аспоряжение'),\n",
    "    re.compile(r'РАСПОРЯЖЕНИЕ'),\n",
    "    re.compile(r'[У|у]каз'),\n",
    "    re.compile(r'УКАЗ')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train/txts/febb5f7a03b7f101bf4dbb644ef00f2daf935c6b.txt', 'r') as f:\n",
    "    doc = f.read()\n",
    "    features = np.array([len(expr.findall(doc)) for expr in reg_exprs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'train/txts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    \"\"\"\n",
    "    X - list of document names\n",
    "    returns list of lists with document features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for doc_name in X:\n",
    "        with open(TRAIN_DIR + doc_name + '.txt', 'r') as f:\n",
    "            doc = f.read()\n",
    "        doc_features = np.array([len(expr.findall(doc)) for expr in reg_exprs])\n",
    "        features.append(doc_features)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2num = {\n",
    "    'федеральный закон': 0,\n",
    "    'постановление': 1,\n",
    "    'приказ': 2,\n",
    "    'распоряжение': 3,\n",
    "    'закон': 4,\n",
    "    'указ': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, y = [], []\n",
    "with open(\"train/gold_labels.txt\", \"r\") as read_file:\n",
    "    for doc_info in read_file.readlines():\n",
    "        doc_dict = json.loads(doc_info)\n",
    "        names.append(doc_dict['id'])\n",
    "        y.append(classes2num[doc_dict['label']['type']])\n",
    "names, y = np.array(names), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extractor = MakeFeatures() \n",
    "X = features_extractor.transform(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98918963, 0.99118965, 0.98150286, 0.9947623 , 0.97904905])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "with open(\"train/gold_labels.txt\", \"r\") as read_file:\n",
    "    for doc_info in read_file.readlines():\n",
    "        doc_dict = json.loads(doc_info)\n",
    "        docname = TRAIN_DIR + doc_dict['id'] + '.txt'\n",
    "        with open(docname, 'r') as f:\n",
    "            train.append((f.read(), doc_dict['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_accuracy': 0.0,\n",
       " 'number_accuracy': 0.0,\n",
       " 'type_f1_score': 0.9838392278,\n",
       " 'name_jaccard': 0.0,\n",
       " 'authority_jaccard': 0.0,\n",
       " 'subtasks_improves': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from task1 import Solution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from eval_module import quality\n",
    "\n",
    "train_data, test = train_test_split(train)\n",
    "solution = Solution()\n",
    "solution.train(train_data)\n",
    "test_data = [x[0] for x in test]\n",
    "test_labels = [x[1] for x in test]\n",
    "result = solution.predict(test_data)\n",
    "quality(result, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
